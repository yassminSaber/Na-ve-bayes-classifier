{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk\n",
    "# ! pip install Wikipedia-API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yasme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "## class 1 (sports)\n",
    "\n",
    "## for trainging\n",
    "football = wiki_wiki.page('Football')\n",
    "basketball = wiki_wiki.page('Basketball')\n",
    "baseball = wiki_wiki.page('Baseball')\n",
    "tennis = wiki_wiki.page('Tennis')\n",
    "volly = wiki_wiki.page('Volleyball')\n",
    "\n",
    "## subset from data \n",
    "football = football.summary[0:650]\n",
    "basketball = basketball.summary[0:650]\n",
    "baseball = baseball.summary[0:650]\n",
    "tennis = tennis.summary[0:650]\n",
    "volly = volly.summary[0:650]\n",
    "\n",
    "## for testing\n",
    "swimming = wiki_wiki.page('Swimming (sport)')\n",
    "swimming = swimming.summary[0:650]\n",
    "\n",
    "## class 2 (programming language)\n",
    "\n",
    "##for trainging\n",
    "python = wiki_wiki.page('Python (programming language)')\n",
    "c_pluse_pluse = wiki_wiki.page('C++')\n",
    "java = wiki_wiki.page('Java (programming language)')\n",
    "c_sharp = wiki_wiki.page('C Sharp (programming language)')\n",
    "c = wiki_wiki.page('C (programming language)')\n",
    "\n",
    "## subset from data \n",
    "python = python.summary[0:750]\n",
    "c_pluse_pluse = c_pluse_pluse.summary[0:750]\n",
    "java = java.summary[0:750]\n",
    "c_sharp = c_sharp.summary[0:750]\n",
    "c = c.summary[0:750]\n",
    "\n",
    "##for testing\n",
    "javascript = wiki_wiki.page('JavaScript')\n",
    "javascript = javascript.summary[0:750]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1 = []\n",
    "class2 = []\n",
    "test1 = []\n",
    "test2 = []\n",
    "\n",
    "class1.append(football)\n",
    "class1.append(basketball)\n",
    "class1.append(baseball)\n",
    "class1.append(tennis)\n",
    "class1.append(volly)\n",
    "\n",
    "class2.append(python)\n",
    "class2.append(c_pluse_pluse)\n",
    "class2.append(c_sharp)\n",
    "class2.append(java)\n",
    "class2.append(c)\n",
    "\n",
    "test1.append(swimming)\n",
    "test2.append(javascript)\n",
    "\n",
    "y_train = 5*[\"sports\"] + 5 *[\"programming language\"]\n",
    "data = class1+class2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. remove punctuatios \n",
    "## 2. split the syntax \n",
    "## 3. dose not remove duplicate words\n",
    "## 4. convert to lower case\n",
    "def prepare_Data(text: list,punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_â€œ~''') -> list:\n",
    "    \n",
    "    for x in text.lower():\n",
    "        if x in punctuations:\n",
    "            text = text.replace(x, \"\")\n",
    "\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.lower()\n",
    "    text = text.split(' ')\n",
    "    text = [x for x in text if x != '']\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "## return unique words in each dictionary\n",
    "def getTokens(text: list) -> list:\n",
    "    words = list(set(text))\n",
    "    words.sort()\n",
    "\n",
    "    unique_word_list = []\n",
    "    for i, word in enumerate(words):\n",
    "      if(word not in unique_word_list):\n",
    "        unique_word_list.append(word)\n",
    "      \n",
    "    return unique_word_list\n",
    "\n",
    "## return stemmed words\n",
    "def stemming(word):\n",
    "  ps = PorterStemmer()\n",
    "  stem_word = ps.stem(word)\n",
    "  return stem_word\n",
    "\n",
    "## return the final list of pre-processed words\n",
    "def preprocessing(Data,string): \n",
    "  preprocessed_data = []\n",
    "  for i in range(len(Data)):\n",
    "    text = prepare_Data(Data[i])\n",
    "    preprocessed_data.append(text)\n",
    "\n",
    "  tokens = []\n",
    "  for i in range(len(preprocessed_data)):\n",
    "    words = getTokens(preprocessed_data[i])\n",
    "    tokens.append(words)\n",
    "\n",
    "  tokens2=[]\n",
    "  for i in range(len(tokens)):\n",
    "    for j in range(len(tokens[i])):\n",
    "      tokens2.append(stemming(tokens[i][j]))\n",
    "\n",
    "  tokens_list = []\n",
    "  for i in range(len(tokens2)):\n",
    "    if(string==\"all\"):\n",
    "      if(tokens2[i] not in tokens_list):\n",
    "        tokens_list.append(tokens2[i])\n",
    "    else:\n",
    "      tokens_list.append(tokens2[i])\n",
    "  return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = preprocessing(data,\"all\") ## all unique words on class1, and class2\n",
    "class1_tokens = preprocessing(class1,\"class\") ## all words in class1\n",
    "class2_tokens = preprocessing(class2,\"class\") ## all words in class2 \n",
    "\n",
    "swim = preprocessing(test1,\"class\") ## all unique words in doc test 1\n",
    "JS = preprocessing(test2,\"class\") ## all unique words in doc test 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate p(ci)\n",
    "def class_prob (doc_label,y_train):\n",
    "  n = len(y_train)\n",
    "  m =0 \n",
    "  for label in y_train :\n",
    "    if label == doc_label :\n",
    "      m = m +1\n",
    "  return (m/n)\n",
    "\n",
    "## calculate the p(word|classi)\n",
    "def conditional_prob(tokens_list,tokens_class_list):\n",
    "  k=len(tokens_list) \n",
    "  class_count = {}\n",
    "  p_w_c = {}\n",
    "  count =0\n",
    "  sum_c_count = 0\n",
    "\n",
    "  ## 1. get classes count\n",
    "  for i in range(len(tokens_list)):\n",
    "    for j in range(len(tokens_class_list)):\n",
    "      if(tokens_list[i]==tokens_class_list[j]):\n",
    "        count = count + 1 \n",
    "    class_count.update({\n",
    "        tokens_list[i]:count\n",
    "    })\n",
    "    count = 0 \n",
    "\n",
    "  ## 2. sum of classes count\n",
    "  for key,value in list(class_count.items()):\n",
    "      sum_c_count = sum_c_count + value\n",
    "  \n",
    "  ## 3. calculate the p(word|classi)\n",
    "  denominator = sum_c_count + k\n",
    "  for i in range(len(tokens_list)):\n",
    "      word = tokens_list[i]\n",
    "      pwc = (class_count[word]+1)/(denominator)\n",
    "      pwc = round(pwc,3)\n",
    "      p_w_c.update({\n",
    "        word : pwc\n",
    "      })\n",
    "\n",
    "  return p_w_c\n",
    "\n",
    "\n",
    "def predict(test_doc,pwc1,pwc2,pc1,pc2,words_c1,word_c2):\n",
    "  ## calculate p(document|classs1) = p(w0|class1)*p(w1|class1)*...*p(wk|class1)\n",
    "  p_d_c1 =1\n",
    "  for i in range(len(test_doc)):\n",
    "    string = test_doc[i]\n",
    "    if string in words_c1:\n",
    "      prop = pwc1[string]\n",
    "      p_d_c1 = p_d_c1 * prop\n",
    "      \n",
    "  ## calculate p(document|class2) = p(w0|class2)*p(w1|class2)*...*p(wk|class2)\n",
    "  p_d_c2 =1\n",
    "  for i in range(len(test_doc)):\n",
    "    string = test_doc[i]\n",
    "    if string in word_c2:\n",
    "      prop = pwc2[string]\n",
    "      p_d_c2 = p_d_c2 * prop\n",
    "  ## p(clas1|document) = p(document|classs1) * p(class1)\n",
    "  p_c1_doc = p_d_c1*pc1\n",
    "  ## p(clas2|document) = p(document|classs2) * p(class2)\n",
    "  p_c2_doc = p_d_c2*pc2\n",
    "  ## final result = p(clas1|document) / p(clas2|document)\n",
    "  result = p_c1_doc /p_c2_doc\n",
    "  \n",
    "  if result > 1:\n",
    "    print(\"sport\")\n",
    "  else:\n",
    "    print(\"programming lanuage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwc1 =conditional_prob(tokens_list,class1_tokens) ## p(w|c1)\n",
    "pwc2 =conditional_prob(tokens_list,class2_tokens) ## p(w|c2)\n",
    "\n",
    "pc1 = class_prob(\"sports\",y_train) # p(c1)\n",
    "pc2 = class_prob(\"programming language\",y_train) # p(c2)\n",
    "\n",
    "words = list(pwc1.keys())  ## words in class1\n",
    "words2 = list(pwc2.keys()) ## words in class2\n",
    "\n",
    "p_w_c = {}\n",
    "for keys,values in list(pwc1.items()):\n",
    "    p_w_c.update({\n",
    "        (\"sport\",keys): values\n",
    "    })\n",
    "\n",
    "for keys,values in list(pwc2.items()):\n",
    "    p_w_c.update({\n",
    "        (\"programming lanuage\",keys): values\n",
    "    })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the probabilties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.getcwd()\n",
    "\n",
    "## save weights to file \n",
    "file = open(\"probabilities.txt\",\"w+\")\n",
    "for item in (p_w_c.items()):\n",
    "    file.write(str(item))\n",
    "    file.write('\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport\n",
      "programming lanuage\n"
     ]
    }
   ],
   "source": [
    "predict(swim,pwc1,pwc2,pc1,pc2,words,words2)\n",
    "predict(JS,pwc1,pwc2,pc1,pc2,words,words2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
